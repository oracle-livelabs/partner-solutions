# Introduction

## About this Workshop

MosaicML was created to enable organizations of any size to develop large AI models efficiently and derive maximum value from their data. In this workshop, you will learn how to use the MosaicML Composer training library and the MosaicML Cloud platform to train a BERT language model from scratch on a custom dataset. Custom-trained models are able to achieve greater accuracy and deliver better results than models that are pre-trained on a generic dataset.

Estimated Workshop Time: 1 hour 30 minutes

### Objectives

At the end of this workshop, you will have an understanding of:
* Launching ML model training jobs on cloud services
* The BERT large language models and some common use cases for them
* Importing a model from the HuggingFace Hub 
* Using MosaicML tools and services to train a BERT model
* Using MosaicML tools and services to fine-tune a pre-trained model

### Prerequisites

This intermediate-level lab assumes that you:
* Have background knowledge or experience with software engineering and/or data science
* Know how to use Jupyter notebooks and JupyterLab
* Feel comfortable reading Python code
* Are familiar with core ML concepts (training, features, loss, inference)
* Have an interest in deep learning

## Learn More

* [Composer Git repository](https://github.com/mosaicml/composer)
* [Composer documentation](https://docs.mosaicml.com/)
* [MosaicML Cloud documentation](https://mcli.docs.mosaicml.com/)
* [MosaicML homepage](https://www.mosaicml.com)

## Acknowledgements
* **Author** - Kobie Crawford, Head of Community, MosaicML
* **Last Updated By/Date** - Kobie Crawford, October 2022
